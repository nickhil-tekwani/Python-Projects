{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f896223e",
   "metadata": {},
   "source": [
    "### Hw 2A || CS 6220 || Nickhil Tekwani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7470b2",
   "metadata": {},
   "source": [
    "## Problem 1: K Means Theory\n",
    "\n",
    "**A) E-Step Update**\n",
    "\n",
    "In the E-step, we fix the centroids ùúá and assign each data point X_i to the nearest centroid.\n",
    "\n",
    "For a given centroid ùúá_k, if it's the closest centroid to the point X_i, then pi_ik = 1. Otherwise, pi_ik = 0.\n",
    "\n",
    "The objective function given the centroids is:\n",
    "\n",
    "$$ J = \\sum_i \\sum_k \\pi_{ik} || X_i - \\mu_k ||^2 $$\n",
    "\n",
    "Given ùúá_k, to minimize \\( J \\) for each \\( X_i \\), we should set pi_ik to 1 for the closest centroid and 0 for all others. Thus, the E-step achieves the minimum objective for the current centroids.\n",
    "\n",
    "**B) M-Step Update**\n",
    "\n",
    "In the M-step, given assignments pi, we need to update the centroids ùúá to minimize the objective. For each cluster \\( k \\), the centroid is updated as the mean of all the data points assigned to that cluster:\n",
    "\n",
    "$$ \\mu_k = \\frac{\\sum_i \\pi_{ik} X_i}{\\sum_i \\pi_{ik}} $$\n",
    "\n",
    "To prove that this choice of ùúá_k minimizes the objective function, consider differentiating the objective with respect to ùúá_k and setting it to zero.\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\mu_k} = 2 \\sum_i \\pi_{ik} (\\mu_k - X_i) $$\n",
    "\n",
    "Setting the above to zero and solving for ùúá_k gives:\n",
    "\n",
    "$$ \\mu_k = \\frac{\\sum_i \\pi_{ik} X_i}{\\sum_i \\pi_{ik}} $$\n",
    "\n",
    "Thus, the M-step update for ùúá_k minimizes the objective for the given memberships pi.\n",
    "\n",
    "**C) Convergence of KMeans**\n",
    "\n",
    "KMeans is guaranteed to converge because the objective function is monotonically non-increasing. In each iteration (combining the E-step and M-step), the objective either decreases or remains the same.\n",
    "\n",
    "However, KMeans might not reach the global minimum of the objective function because the algorithm can get stuck in local minima. This behavior is influenced by the initial choice of centroids. Different runs of KMeans with different initializations can lead to different final cluster assignments and centroids.\n",
    "\n",
    "The possibility of local minima is why it's common to run KMeans multiple times with different initializations and choose the run that results in the lowest objective value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c20832",
   "metadata": {},
   "source": [
    "## Problem 2: K Means on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71569a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k, distance=\"euclidean\", max_iters=100, tol=1e-4):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.centroids = None\n",
    "        self.distance = distance\n",
    "    \n",
    "    def _calculate_distance(self, x1, x2):\n",
    "        if self.distance == \"euclidean\":\n",
    "            return np.linalg.norm(x1 - x2)\n",
    "        elif self.distance == \"dot\":\n",
    "            return -np.dot(x1, x2)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown distance metric!\")\n",
    "            \n",
    "    def fit(self, X):\n",
    "        # Initialize centroids randomly from the data points\n",
    "        self.centroids = X[np.random.choice(X.shape[0], self.k, replace=False)]\n",
    "        prev_centroids = np.zeros_like(self.centroids)\n",
    "        \n",
    "        for _ in range(self.max_iters):\n",
    "            distances = np.array([[self._calculate_distance(x, centroid) for centroid in self.centroids] for x in X])\n",
    "            clusters = np.argmin(distances, axis=1)\n",
    "            \n",
    "            # Update Centroids\n",
    "            new_centroids = np.array([X[clusters == i].mean(axis=0) for i in range(self.k)])\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(new_centroids - prev_centroids) < self.tol:\n",
    "                break\n",
    "            \n",
    "            prev_centroids = new_centroids\n",
    "            self.centroids = new_centroids\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = np.array([[self._calculate_distance(x, centroid) for centroid in self.centroids] for x in X])\n",
    "        return np.argmin(distances, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "939d5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def kmeans_objective(X, labels, centroids):\n",
    "    return sum(np.linalg.norm(X[i] - centroids[labels[i]])**2 for i in range(X.shape[0]))\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # Convert both sets of labels to string type\n",
    "    y_true_str = [str(label) for label in y_true]\n",
    "    y_pred_str = [str(label) for label in y_pred]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    matrix = confusion_matrix(y_true_str, y_pred_str)\n",
    "    \n",
    "    # Return purity\n",
    "    return np.sum(np.amax(matrix, axis=0)) / np.sum(matrix)\n",
    "\n",
    "def gini_index(y_true, y_pred):\n",
    "    # Convert both sets of labels to string type\n",
    "    y_true_str = [str(label) for label in y_true]\n",
    "    y_pred_str = [str(label) for label in y_pred]\n",
    "    \n",
    "    matrix = confusion_matrix(y_true_str, y_pred_str)\n",
    "    total_samples = np.sum(matrix)\n",
    "    sum_gini = 0\n",
    "    \n",
    "    # Iterate over each cluster\n",
    "    for cluster in matrix.T:  # Transpose to get clusters\n",
    "        # Proportions of each class in the cluster\n",
    "        proportions = cluster / np.sum(cluster)\n",
    "        gini = 1 - np.sum(proportions ** 2)\n",
    "        \n",
    "        # Weighted Gini (by the size of the cluster)\n",
    "        sum_gini += gini * np.sum(cluster)\n",
    "\n",
    "    return sum_gini / total_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57cbb1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def load_data(name, n_samples=5000, n_features=30):\n",
    "    if name == \"MNIST\":\n",
    "        data = fetch_openml('mnist_784', cache=True)\n",
    "        X, y = data[\"data\"].values, data[\"target\"].values\n",
    "    elif name == \"FASHION\":\n",
    "        data = fetch_openml('Fashion-MNIST', cache=True)\n",
    "        X, y = data[\"data\"].values, data[\"target\"].values\n",
    "    elif name == \"20NG\":\n",
    "        newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "        vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "        X = vectorizer.fit_transform(newsgroups.data)\n",
    "        y = newsgroups.target\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset!\")\n",
    "    \n",
    "    if name == \"20NG\":\n",
    "        reducer = TruncatedSVD(n_components=n_features)\n",
    "    else:\n",
    "        reducer = PCA(n_components=n_features)\n",
    "        \n",
    "    X_reduced = reducer.fit_transform(X[:n_samples])\n",
    "    \n",
    "    return X_reduced, y[:n_samples]\n",
    "\n",
    "mnist_data, mnist_labels = load_data(\"MNIST\")\n",
    "fashion_data, fashion_labels = load_data(\"FASHION\")\n",
    "ng_data, ng_labels = load_data(\"20NG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b059380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/kjz0bk3s5nj4p4v10t35f8f40000gn/T/ipykernel_86639/1439885653.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  proportions = cluster / np.sum(cluster)\n",
      "/var/folders/2d/kjz0bk3s5nj4p4v10t35f8f40000gn/T/ipykernel_86639/1439885653.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  proportions = cluster / np.sum(cluster)\n",
      "/var/folders/2d/kjz0bk3s5nj4p4v10t35f8f40000gn/T/ipykernel_86639/1439885653.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  proportions = cluster / np.sum(cluster)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MNIST': {'Objective': 8077677453.634104, 'Purity': 0.569, 'Gini Index': 0.5596332793331051}, 'MNIST_half': {'Objective': 9482325314.473421, 'Purity': 0.452, 'Gini Index': nan}, 'MNIST_double': {'Objective': 6720295577.77635, 'Purity': 0.7106, 'Gini Index': 0.40856988453190235}, 'FASHION': {'Objective': 6507857495.657722, 'Purity': 0.5398, 'Gini Index': 0.5649251411556635}, 'FASHION_half': {'Objective': 8819454308.01257, 'Purity': 0.4188, 'Gini Index': nan}, 'FASHION_double': {'Objective': 4807882275.618217, 'Purity': 0.65, 'Gini Index': 0.45012659919913156}, '20NG': {'Objective': 270.58083294772496, 'Purity': 0.2878, 'Gini Index': 0.7968240834059621}, '20NG_half': {'Objective': 331.15340506797617, 'Purity': 0.2166, 'Gini Index': nan}, '20NG_double': {'Objective': 219.42475357447225, 'Purity': 0.319, 'Gini Index': 0.770127727306173}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datasets = {\n",
    "    \"MNIST\": (mnist_data, mnist_labels, 10),\n",
    "    \"FASHION\": (fashion_data, fashion_labels, 10),\n",
    "    \"20NG\": (ng_data, ng_labels, 20)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, (data, labels, k) in datasets.items():\n",
    "    km = KMeans(k=k)\n",
    "    km.fit(data)\n",
    "    preds = km.predict(data)\n",
    "    \n",
    "    results[name] = {\n",
    "        \"Objective\": kmeans_objective(data, preds, km.centroids),\n",
    "        \"Purity\": purity_score(labels, preds),\n",
    "        \"Gini Index\": gini_index(labels, preds)\n",
    "    }\n",
    "\n",
    "    # Additional runs for higher and lower K values\n",
    "    for factor, multiplier in {\"half\": 0.5, \"double\": 2}.items():\n",
    "        km = KMeans(k=int(k * multiplier))\n",
    "        km.fit(data)\n",
    "        preds = km.predict(data)\n",
    "\n",
    "        results[f\"{name}_{factor}\"] = {\n",
    "            \"Objective\": kmeans_objective(data, preds, km.centroids),\n",
    "            \"Purity\": purity_score(labels, preds),\n",
    "            \"Gini Index\": gini_index(labels, preds)\n",
    "        }\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f720885",
   "metadata": {},
   "source": [
    "### PROBLEM 3 : Gaussian Mixture on toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e09ebd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 2-gaussian.txt\n",
      "Means: [[2.99413182 3.0520966 ]\n",
      " [7.01314831 3.98313418]]\n",
      "Covariances: [array([[1.01023425, 0.02719139],\n",
      "       [0.02719139, 2.93782297]]), array([[0.97475893, 0.49747031],\n",
      "       [0.49747031, 1.0011426 ]])]\n",
      "Weights: [0.33479577 0.66520423]\n",
      "\n",
      "Results for 3-gaussian.txt\n",
      "Means: [[5.01177664 7.00151504]\n",
      " [3.03980012 3.04879137]\n",
      " [7.02158525 4.01547353]]\n",
      "Covariances: [array([[0.97965466, 0.18512102],\n",
      "       [0.18512102, 0.97448689]]), array([[1.02858546, 0.02702432],\n",
      "       [0.02702432, 3.38530379]]), array([[0.99037288, 0.50094401],\n",
      "       [0.50094401, 0.99564696]])]\n",
      "Weights: [0.49594551 0.20562097 0.29843352]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def initialize_parameters(data, n_components):\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    n_samples, _ = data.shape\n",
    "    shuffled_indices = np.random.permutation(n_samples)\n",
    "    means = data[shuffled_indices[:n_components]]\n",
    "    covariances = [np.cov(data, rowvar=False)] * n_components\n",
    "    weights = [1./n_components] * n_components\n",
    "    return means, covariances, weights\n",
    "\n",
    "def e_step(data, means, covs, weights):\n",
    "    n_samples, n_features = data.shape\n",
    "    n_components = len(weights)\n",
    "    resp = np.zeros((n_samples, n_components))\n",
    "    for i in range(n_components):\n",
    "        resp[:, i] = weights[i] * multivariate_normal(mean=means[i], cov=covs[i]).pdf(data)\n",
    "    resp /= resp.sum(axis=1)[:, np.newaxis]\n",
    "    return resp\n",
    "\n",
    "def m_step(data, resp):\n",
    "    n_samples, n_features = data.shape\n",
    "    n_components = resp.shape[1]\n",
    "    weights = resp.sum(axis=0) / n_samples\n",
    "    means = np.dot(resp.T, data) / resp.sum(axis=0)[:, np.newaxis]\n",
    "    covariances = []\n",
    "    for i in range(n_components):\n",
    "        diff = (data - means[i]).T\n",
    "        cov = np.dot(resp[:, i] * diff, diff.T) / resp[:, i].sum()\n",
    "        covariances.append(cov)\n",
    "    return means, covariances, weights\n",
    "\n",
    "def compute_log_likelihood(data, means, covs, weights):\n",
    "    n_samples = data.shape[0]\n",
    "    n_components = len(weights)\n",
    "    log_likelihood = np.zeros((n_samples, n_components))\n",
    "    for i in range(n_components):\n",
    "        log_likelihood[:, i] = np.log(weights[i]) + multivariate_normal(mean=means[i], cov=covs[i]).logpdf(data)\n",
    "    return log_likelihood.sum()\n",
    "\n",
    "def em_gmm(data, n_components, max_iter=100, tol=1e-4):\n",
    "    means, covs, weights = initialize_parameters(data, n_components)\n",
    "    log_likelihood = -np.inf\n",
    "    for i in range(max_iter):\n",
    "        resp = e_step(data, means, covs, weights)\n",
    "        means, covs, weights = m_step(data, resp)\n",
    "        new_log_likelihood = compute_log_likelihood(data, means, covs, weights)\n",
    "        if np.abs(new_log_likelihood - log_likelihood) < tol:\n",
    "            break\n",
    "        log_likelihood = new_log_likelihood\n",
    "    return means, covs, weights\n",
    "\n",
    "# Load data\n",
    "data_2gaussian = np.loadtxt('2gaussian.txt')\n",
    "data_3gaussian = np.loadtxt('3gaussian.txt')\n",
    "\n",
    "# Run EM for 2-Gaussian data\n",
    "means_2g, covs_2g, weights_2g = em_gmm(data_2gaussian, 2)\n",
    "print(\"Results for 2-gaussian.txt\")\n",
    "print(\"Means:\", means_2g)\n",
    "print(\"Covariances:\", covs_2g)\n",
    "print(\"Weights:\", weights_2g)\n",
    "\n",
    "# Run EM for 3-Gaussian data\n",
    "means_3g, covs_3g, weights_3g = em_gmm(data_3gaussian, 3)\n",
    "print(\"\\nResults for 3-gaussian.txt\")\n",
    "print(\"Means:\", means_3g)\n",
    "print(\"Covariances:\", covs_3g)\n",
    "print(\"Weights:\", weights_3g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f3362",
   "metadata": {},
   "source": [
    "### PROBLEM 4 EM for coin flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "213cbc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_A, p_B, p_C: [0.9317285  0.23691765 0.610037  ]\n",
      "pi_A, pi_B, pi_C: [0.1785578  0.30681208 0.51463012]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Load the data from the file\n",
    "with open('coin_flips_outcome.txt', 'r') as file:\n",
    "    data = [list(map(int, line.strip().split())) for line in file.readlines()]\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "N, D = data.shape\n",
    "\n",
    "# Initialize parameters\n",
    "p = np.random.rand(3)\n",
    "pi = np.random.rand(3)\n",
    "pi /= pi.sum()\n",
    "\n",
    "def e_step(data, p, pi):\n",
    "    weights = np.zeros((N, 3))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(3):\n",
    "            weights[i, j] = pi[j] * binom.pmf(np.sum(data[i]), D, p[j])\n",
    "    \n",
    "    weights /= weights.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def m_step(data, weights):\n",
    "    p = np.sum(weights * np.sum(data, axis=1, keepdims=True), axis=0) / (D * np.sum(weights, axis=0))\n",
    "    pi = np.mean(weights, axis=0)\n",
    "    return p, pi\n",
    "\n",
    "# EM algorithm\n",
    "max_iterations = 1000\n",
    "tolerance = 1e-6\n",
    "for iteration in range(max_iterations):\n",
    "    # E-step\n",
    "    weights = e_step(data, p, pi)\n",
    "    \n",
    "    # M-step\n",
    "    p_new, pi_new = m_step(data, weights)\n",
    "    \n",
    "    # Check for convergence\n",
    "    if np.max(abs(p_new - p)) < tolerance and np.max(abs(pi_new - pi)) < tolerance:\n",
    "        break\n",
    "    \n",
    "    p, pi = p_new, pi_new\n",
    "\n",
    "print(\"p_A, p_B, p_C:\", p)\n",
    "print(\"pi_A, pi_B, pi_C:\", pi)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
