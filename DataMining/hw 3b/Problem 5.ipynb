{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nickhil Tekwani Hw3b Problem 5"
      ],
      "metadata": {
        "id": "BmuMKLfh835v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl-aDYpg5WXu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def generate_rectangles(num_rect=100):\n",
        "    rectangles = []\n",
        "    while len(rectangles) < num_rect:\n",
        "        w = np.random.randint(5, 24)  # Max width set to 23 to ensure 28-w is positive\n",
        "        h = int(130/w) + 1\n",
        "        if h > 23:  # Ensuring that height is also within the bounds\n",
        "            continue\n",
        "\n",
        "        x = np.random.randint(0, 28 - w)\n",
        "        y = np.random.randint(0, 28 - h)\n",
        "\n",
        "        rectangles.append((x, y, w, h))\n",
        "    return rectangles\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_integral_image(image):\n",
        "    integral = np.zeros_like(image, dtype=np.int)\n",
        "    for i in range(image.shape[0]):\n",
        "        for j in range(image.shape[1]):\n",
        "            integral[i, j] = image[i, j]\n",
        "            if i > 0:\n",
        "                integral[i, j] += integral[i-1, j]\n",
        "            if j > 0:\n",
        "                integral[i, j] += integral[i, j-1]\n",
        "            if i > 0 and j > 0:\n",
        "                integral[i, j] -= integral[i-1, j-1]\n",
        "    return integral\n",
        "\n",
        "def get_black_value(integral, x, y, w, h):\n",
        "    total = integral[y+h-1, x+w-1]\n",
        "    if x > 0:\n",
        "        total -= integral[y+h-1, x-1]\n",
        "    if y > 0:\n",
        "        total -= integral[y-1, x+w-1]\n",
        "    if x > 0 and y > 0:\n",
        "        total += integral[y-1, x-1]\n",
        "    return total\n",
        "\n",
        "def compute_haar_features(image, rectangles):\n",
        "    integral = compute_integral_image(image)\n",
        "    features = []\n",
        "\n",
        "    for rect in rectangles:\n",
        "        x, y, w, h = rect\n",
        "        h_mid = h // 2\n",
        "        w_mid = w // 2\n",
        "\n",
        "        vertical_feature = get_black_value(integral, x, y, w, h_mid) - get_black_value(integral, x, y+h_mid, w, h_mid)\n",
        "        horizontal_feature = get_black_value(integral, x, y, w_mid, h) - get_black_value(integral, x+w_mid, y, w_mid, h)\n",
        "\n",
        "        features.extend([vertical_feature, horizontal_feature])\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "2fw0cHIl56Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load MNIST data\n",
        "mnist = fetch_openml('mnist_784')\n",
        "X, y = mnist.data.values, mnist.target.values\n",
        "\n",
        "# Normalize data\n",
        "X = X / 255.0\n",
        "X = X.reshape(-1, 28, 28)\n",
        "\n",
        "# Generate 100 random rectangles\n",
        "rectangles = generate_rectangles()\n",
        "\n",
        "# Extract HAAR features\n",
        "X_features = [compute_haar_features(img, rectangles) for img in X]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Classification using RandomForest\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUjvY39d5-ZC",
        "outputId": "ad8adb31-214b-4589-acb4-2f7a1eb7e535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n",
            "<ipython-input-2-9fdd5284ae1d>:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  integral = np.zeros_like(image, dtype=np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4266\n"
          ]
        }
      ]
    }
  ]
}