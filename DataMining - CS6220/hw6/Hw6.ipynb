{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nickhil Tekwani CS6220 Hw6"
      ],
      "metadata": {
        "id": "H_ruQjBdRT96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 1: Recommender System using Collaborative Filtering"
      ],
      "metadata": {
        "id": "o6P6sfyqT0oA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "id": "lZrhL61AP5jQ",
        "outputId": "65b74b39-3941-4612-cb84-5e239fbba73f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Items in test but not in user_item_matrix: 29\n",
            "Items in test but not in similarity_matrix: 29\n",
            "User: 655, Item: 1640 not found in the matrices.\n",
            "User: 425, Item: 1596 not found in the matrices.\n",
            "User: 60, Item: 1122 not found in the matrices.\n",
            "User: 206, Item: 1433 not found in the matrices.\n",
            "User: 854, Item: 1677 not found in the matrices.\n",
            "User: 181, Item: 1320 not found in the matrices.\n",
            "User: 405, Item: 1565 not found in the matrices.\n",
            "User: 405, Item: 1546 not found in the matrices.\n",
            "User: 450, Item: 1603 not found in the matrices.\n",
            "User: 279, Item: 1493 not found in the matrices.\n",
            "User: 782, Item: 1669 not found in the matrices.\n",
            "User: 405, Item: 1551 not found in the matrices.\n",
            "User: 682, Item: 1655 not found in the matrices.\n",
            "User: 181, Item: 1352 not found in the matrices.\n",
            "User: 655, Item: 1648 not found in the matrices.\n",
            "User: 234, Item: 1460 not found in the matrices.\n",
            "User: 445, Item: 1601 not found in the matrices.\n",
            "User: 437, Item: 1599 not found in the matrices.\n",
            "User: 13, Item: 814 not found in the matrices.\n",
            "User: 916, Item: 1682 not found in the matrices.\n",
            "User: 655, Item: 1649 not found in the matrices.\n",
            "User: 840, Item: 1674 not found in the matrices.\n",
            "User: 655, Item: 1632 not found in the matrices.\n",
            "User: 181, Item: 1364 not found in the matrices.\n",
            "User: 181, Item: 1349 not found in the matrices.\n",
            "User: 385, Item: 1536 not found in the matrices.\n",
            "User: 787, Item: 1433 not found in the matrices.\n",
            "User: 851, Item: 1675 not found in the matrices.\n",
            "User: 655, Item: 1641 not found in the matrices.\n",
            "User: 655, Item: 1637 not found in the matrices.\n",
            "User: 456, Item: 1551 not found in the matrices.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-59b9a3026325>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Calculate RMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rating\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predicted_rating\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Root Mean Squared Error (RMSE): {rmse}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;36m0.825\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "col_names = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
        "ratings = pd.read_csv(\"/content/u.data\", sep=\"\\t\", names=col_names)\n",
        "\n",
        "# Train-test split\n",
        "train, test = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create user-item matrix\n",
        "user_item_matrix = train.pivot(index=\"user_id\", columns=\"item_id\", values=\"rating\")\n",
        "\n",
        "# User-User Similarity Matrix\n",
        "similarity_matrix = user_item_matrix.corr()\n",
        "def predict_rating(user_id, item_id):\n",
        "    \"\"\"Predicts a user's rating for an item.\"\"\"\n",
        "\n",
        "    # Check if the user or item doesn't exist in the matrices\n",
        "    if user_id not in user_item_matrix.index or item_id not in user_item_matrix.columns:\n",
        "        print(f\"User: {user_id}, Item: {item_id} not found in the matrices.\")\n",
        "        return user_item_matrix.values.mean()  # Fallback to the global average rating\n",
        "\n",
        "    # Otherwise, compute prediction using the user-item and similarity matrix\n",
        "    # You may use dot product of user's ratings and item's similarity scores\n",
        "    # and divide by the sum of absolute similarities.\n",
        "\n",
        "    # Ensure the similarity scores for the item are available\n",
        "    if item_id in similarity_matrix.columns:\n",
        "        item_similarity = similarity_matrix[item_id]\n",
        "        user_ratings = user_item_matrix.loc[user_id]\n",
        "\n",
        "        # Compute the dot product\n",
        "        dot_product = user_ratings.dot(item_similarity)\n",
        "\n",
        "        # Calculate the predicted rating\n",
        "        predicted_rating_value = dot_product / item_similarity.abs().sum()\n",
        "\n",
        "        return predicted_rating_value\n",
        "\n",
        "    # Fallback to the user's average rating if the above computation fails\n",
        "    return user_item_matrix.loc[user_id].mean()\n",
        "\n",
        "\n",
        "\n",
        "missing_items_user_item = set(test['item_id'].unique()) - set(user_item_matrix.columns)\n",
        "missing_items_similarity = set(test['item_id'].unique()) - set(similarity_matrix.columns)\n",
        "\n",
        "print(f\"Items in test but not in user_item_matrix: {len(missing_items_user_item)}\")\n",
        "print(f\"Items in test but not in similarity_matrix: {len(missing_items_similarity)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Predict ratings on the test set\n",
        "test[\"predicted_rating\"] = test.apply(lambda x: predict_rating(x[\"user_id\"], x[\"item_id\"]), axis=1)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(test[\"rating\"], test[\"predicted_rating\"]))\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 3A: Social Community Detection\n"
      ],
      "metadata": {
        "id": "VJiR6uXkTzkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import igraph as ig\n",
        "\n",
        "def girvan_newman(graph, iterations=None):\n",
        "    \"\"\"\n",
        "    Implement the Girvan-Newman algorithm to detect communities.\n",
        "    \"\"\"\n",
        "    # Deep copy of the graph to ensure original remains unchanged\n",
        "    g = graph.copy()\n",
        "\n",
        "    # Keeps track of the graphs at each split\n",
        "    sub_graphs = [g]\n",
        "\n",
        "    # Either iterate until graph is fully dissected or for a set number of iterations\n",
        "    i = 0\n",
        "    while len(sub_graphs) == i + 1:\n",
        "        if iterations and i >= iterations:\n",
        "            break\n",
        "\n",
        "        # Compute the edge betweenness scores\n",
        "        betweenness = g.edge_betweenness()\n",
        "\n",
        "        # Find the edge with the highest betweenness score\n",
        "        max_betweenness = max(betweenness)\n",
        "        max_index = betweenness.index(max_betweenness)\n",
        "\n",
        "        # Remove the edge with the highest betweenness\n",
        "        g.delete_edges(max_index)\n",
        "\n",
        "        # Get the subgraphs (i.e., the connected components) of the graph\n",
        "        sub_graphs = g.decompose()\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    # Return the resulting subgraphs as communities\n",
        "    return sub_graphs\n",
        "\n",
        "def read_edgelist(filename):\n",
        "    \"\"\"Read an edge list from a file and return an igraph.Graph.\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        edges = [tuple(map(int, line.strip().split(','))) for line in f]\n",
        "    return ig.Graph(edges, directed=False)\n",
        "\n",
        "\n",
        "# Use the custom function to read the graph\n",
        "g = read_edgelist('/content/soc-Flickr-ASU.edges')\n",
        "\n",
        "# Read the node labels\n",
        "with open('/content/soc-Flickr-ASU.node_labels', 'r') as f:\n",
        "    labels_data = [line.strip().split(',') for line in f]\n",
        "    node_ids = [int(data[0]) for data in labels_data]\n",
        "    labels = [data[1] for data in labels_data]\n",
        "\n",
        "# Assign a 'name' attribute for each vertex based on its ID\n",
        "g.vs[\"name\"] = [v.index for v in g.vs]\n",
        "\n",
        "# Map labels to the graph nodes\n",
        "for idx, label in zip(node_ids, labels):\n",
        "    try:\n",
        "        node = g.vs.find(name=idx)  # Use integer idx directly without converting to string\n",
        "        node[\"label\"] = label\n",
        "    except ValueError:\n",
        "        # The node does not exist in the graph, so we ignore it\n",
        "        pass\n",
        "\n",
        "\n",
        "# Call the Girvan-Newman function\n",
        "communities = girvan_newman(g, 10)\n",
        "\n",
        "# Print the communities\n",
        "for community in communities:\n",
        "    print(community.vs[\"label\"])\n"
      ],
      "metadata": {
        "id": "e6em2caGe03a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}